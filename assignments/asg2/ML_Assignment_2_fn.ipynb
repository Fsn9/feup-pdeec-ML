{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32620509",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1993cc",
   "metadata": {},
   "source": [
    "Student: Francisco Neves (up201404576)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03c866e",
   "metadata": {},
   "source": [
    "## 1. Generative classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38343624",
   "metadata": {},
   "source": [
    "Consider a classification problem with a target variable $y \\in \\{0, 1\\}$ and input features $\\boldsymbol{x} = (x_1\\; x_2\\; x_3\\; x_4)^T$, where $x_1 \\in \\{0, 1\\}$, $x_2 \\in \\{0, 1\\}$, and $(x_3, x_4) \\in \\mathbb{R}^2$. Further assume that:\n",
    "- $(x_1, x_2)$ is conditionally independent of $(x_3, x_4)$ given $y$;\n",
    "- $x_1$ and $x_2$ are **dependent** given $y$;\n",
    "- $x_3$ and $x_4$ are **dependent** given $y$;\n",
    "- the conditional distributions of $(x_3, x_4)$ given $y$ are Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c2977c",
   "metadata": {},
   "source": [
    "### 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6081895",
   "metadata": {},
   "source": [
    "**a)** Enumerate the parameters of the MAP classifier: $$\\hat{y} = \\text{arg} \\max_{y \\in \\{0, 1\\}} p(y)p(\\boldsymbol{x} \\mid y),$$ and indicate the dimension of each parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc08105",
   "metadata": {},
   "source": [
    "___\n",
    "Before estimating the parameters, by assuming partial independence between $(x_1,x_2)$ and $(x_3,x_4)$ the likelihood is described by:\n",
    "$\\begin{equation}\n",
    "p\\left(\\begin{bmatrix}\n",
    "x_{1} \\\\\n",
    "x_{2} \\\\\n",
    "x_{3} \\\\\n",
    "x_{4}\n",
    "\\end{bmatrix}\\middle\\vert y\\right) = \n",
    "p\\left(\\begin{bmatrix}\n",
    "x_{1} \\\\\n",
    "x_{2}\n",
    "\\end{bmatrix}\\middle\\vert y\\right)\n",
    "p\\left(\\begin{bmatrix}\n",
    "x_{3} \\\\\n",
    "x_{4}\n",
    "\\end{bmatrix}\\middle\\vert y\\right).\n",
    "\\end{equation}$\n",
    "\n",
    "Following, we enumerate the different parameters by breaking the problem in three parts (a), (b) and (c).\n",
    "### (a) Parameters for $(x_1,x_2)$\n",
    "For the jointed $(x_1, x_2)$ features, we have six parameters in total: 3 parameters for $p\\left(\\begin{bmatrix} x_{1} \\\\ x_{2}\\end{bmatrix}\\middle\\vert y = 0 \\right)$ and 3 parameters for $p\\left(\\begin{bmatrix} x_{1} \\\\ x_{2}\\end{bmatrix}\\middle\\vert y = 1\\right)$. Since $x_1 \\in \\{0,1\\}$ and $x_2 \\in \\{0,1\\}$ and they are dependent between each other given $y$, one needs to consider all the possible four joint configuration using 2 bits of information, such as $[0,0], [0,1], [1,0], [1,1]$. At least three parameters are enough to estimate for each $p\\left(\\begin{bmatrix} x_{1} \\\\ x_{2}\\end{bmatrix}\\middle\\vert y_i\\right)$, since a fourth is indirectly deduced from the others, such as,\n",
    "$\\begin{equation}\n",
    "  p\\left(\\begin{bmatrix} x_{1} = 0 \\\\ x_{2} = 0\\end{bmatrix}\\middle\\vert y_i\\right) + p\\left(\\begin{bmatrix} x_{1} = 0 \\\\ x_{2} = 1\\end{bmatrix}\\middle\\vert y_i \\right) + p\\left(\\begin{bmatrix} x_{1} = 1 \\\\ x_{2} = 0\\end{bmatrix}\\middle\\vert y_i \\right) + p\\left(\\begin{bmatrix} x_{1} = 1 \\\\ x_{2} = 1\\end{bmatrix}\\middle\\vert y_i \\right) = 1,\n",
    "\\end{equation}$\n",
    "where if $p\\left(\\begin{bmatrix} x_{1} = 1 \\\\ x_{2} = 1\\end{bmatrix}\\middle\\vert y_i \\right)$ is the one not considered, the computation is as follows:\n",
    "$\\begin{equation}\n",
    "p\\left(\\begin{bmatrix} x_{1} = 1 \\\\ x_{2} = 1\\end{bmatrix}\\middle\\vert y_i \\right) = 1 - \\left(p\\left(\\begin{bmatrix} x_{1} = 0 \\\\ x_{2} = 0\\end{bmatrix}\\middle\\vert y_i\\right) + p\\left(\\begin{bmatrix} x_{1} = 0 \\\\ x_{2} = 1\\end{bmatrix}\\middle\\vert y_i \\right) + p\\left(\\begin{bmatrix} x_{1} = 1 \\\\ x_{2} = 0\\end{bmatrix}\\middle\\vert y_i \\right)\\right).\n",
    "\\end{equation}$ \n",
    "Hence, the six parameters are the following:\n",
    "* 3 parameters for $p\\left(\\begin{bmatrix} x_{1} \\\\ x_{2}\\end{bmatrix}\\middle\\vert y = 0 \\right)$: \n",
    "  * $p\\left(\\begin{bmatrix} x_{1} = 0 \\\\ x_{2} = 0\\end{bmatrix}\\middle\\vert y = 0 \\right)$\n",
    "  * $p\\left(\\begin{bmatrix} x_{1} = 0 \\\\ x_{2} = 1\\end{bmatrix}\\middle\\vert y = 0 \\right)$\n",
    "  * $p\\left(\\begin{bmatrix} x_{1} = 1 \\\\ x_{2} = 0\\end{bmatrix}\\middle\\vert y = 0 \\right)$\n",
    "* 3 parameters for $p\\left(\\begin{bmatrix} x_{1} \\\\ x_{2}\\end{bmatrix}\\middle\\vert y = 1 \\right)$:\n",
    "  * $p\\left(\\begin{bmatrix} x_{1} = 0 \\\\ x_{2} = 0\\end{bmatrix}\\middle\\vert y = 1 \\right)$\n",
    "  * $p\\left(\\begin{bmatrix} x_{1} = 0 \\\\ x_{2} = 1\\end{bmatrix}\\middle\\vert y = 1 \\right)$\n",
    "  * $p\\left(\\begin{bmatrix} x_{1} = 1 \\\\ x_{2} = 0\\end{bmatrix}\\middle\\vert y = 1 \\right)$\n",
    "\n",
    "\n",
    "### (b) Parameters for $(x_3, x_4)$\n",
    "For the jointed $(x_3, x_4)$ features, we have ten parameters, as follows: \n",
    "* 4 means: $\\mu_{x_3}=\\begin{bmatrix}\\mu_{x_3}^{y=0} \\\\ \\mu_{x_3}^{y=1}\\end{bmatrix}$ and $\\mu_{x_4}=\\begin{bmatrix}\\mu_{x_4}^{y=0} \\\\ \\mu_{x_4}^{y=1}\\end{bmatrix}$\n",
    "* 6 covariance elements: $\\begin{bmatrix}\n",
    "\\sigma(x_3,x_3)^{2}_{y = 0} & \\sigma(x_3,x_4)^{2}_{y = 0} \\\\\n",
    "\\cdots & \\sigma(x_4,x_4)^{2}_{y = 0}\n",
    "\\end{bmatrix}$ and \n",
    "$ \\begin{bmatrix}\n",
    "\\sigma(x_3,x_3)^{2}_{y = 1} & \\sigma(x_3,x_4)^{2}_{y = 1} \\\\\n",
    "\\cdots & \\sigma(x_4,x_4)^{2}_{y = 1}\n",
    "\\end{bmatrix}$, where $ \\sigma(x_4,x_3)^{2}_{y = 1}$ and  $\\sigma(x_4,x_3)^{2}_{y = 0}$ are omitted, since  $\\sigma(x_3,x_4)^{2}_{y = 0} = \\sigma(x_4,x_3)^{2}_{y = 0}$ and $\\sigma(x_3,x_4)^{2}_{y = 1} = \\sigma(x_4,x_3)^{2}_{y = 1}$ because the covariance matrix is symmetric.\n",
    "\n",
    "### (c) Prior parameters\n",
    "Finally, we have to estimate just one prior, e.g,: $p(y=0)$. Since $p(y=0) = 1 - p(y=1)$, knowing one of them deduces the other.\n",
    "\n",
    "### Number of parameters\n",
    "Hence, in total the model has 17 parameters, such as: 6 parameters from the part (a) plus 10 parameters from the part (b) plus 1 parameter from the part (c).\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41873490",
   "metadata": {},
   "source": [
    "**b)** Given a dataset $\\{(\\boldsymbol{x}^{(i)}, y^{(i)})\\}_{i=1}^n$, write the expressions for the maximum likelihood estimates of the parameters enumerated in the previous question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2a6e5d",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE* (either typeset or a digitalization of your handwritten answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c45eaf5",
   "metadata": {},
   "source": [
    "### 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba5cb63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9823efc7",
   "metadata": {},
   "source": [
    "Now, you will implement this classifier in Python. The classifier skeleton is provided below in the class `Classifier`. You may implement additional auxiliary methods that you find useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "12ed9dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Inputs:\n",
    "            X - np.array with shape (num_examples_train, 4)\n",
    "            y - np.array with shape (num_examples_train,)\n",
    "        '''\n",
    "        # Class attributes\n",
    "        self.__priors = {}\n",
    "        self.__mus = {}\n",
    "        self.__covs = {}\n",
    "\n",
    "        # Useful variables\n",
    "        Xy = np.concatenate((X,y.reshape(-1,1)), axis = 1)\n",
    "        target = Xy[:,-1]\n",
    "        N = Xy.shape[0]\n",
    "        X_0, X_1 = Xy[target == 0], Xy[target == 1]\n",
    "        x00_0 = X_0[(X_0[:,0] == 0) & (X_0[:,1] == 0)]\n",
    "        x01_0 = X_0[(X_0[:,0] == 0) & (X_0[:,1] == 1)]\n",
    "        x10_0 = X_0[(X_0[:,0] == 1) & (X_0[:,1] == 0)]\n",
    "        x11_0 = X_0[(X_0[:,0] == 1) & (X_0[:,1] == 1)]\n",
    "        x00_1 = X_1[(X_1[:,0] == 0) & (X_1[:,1] == 0)]\n",
    "        x01_1 = X_1[(X_1[:,0] == 0) & (X_1[:,1] == 1)]\n",
    "        x10_1 = X_1[(X_1[:,0] == 1) & (X_1[:,1] == 0)]\n",
    "        x01_1 = X_1[(X_1[:,0] == 1) & (X_1[:,1] == 1)]\n",
    "        x3_0, x3_1, x4_0, x4_1 = X_0[:,2].reshape(-1,1), X_1[:,2].reshape(-1,1), X_0[:,3].reshape(-1,1), X_1[:,3].reshape(-1,1)\n",
    "\n",
    "        # (a) Conditional discrete probabilities for x1 and x2\n",
    "        p00_0, p01_0, p10_0 = x00_0.shape[0] / X_0.shape[0], x01_0.shape[0] / X_0.shape[0], x10_0.shape[0] / X_0.shape[0]\n",
    "        p11_0 = 1 - (p00_0 + p01_0 + p10_0)\n",
    "        p00_1, p01_1, p10_1 = x00_1.shape[0] / X_1.shape[0], x01_1.shape[0] / X_1.shape[0], x10_1.shape[0] / X_1.shape[0]\n",
    "        p11_1 = 1 - (p00_1 + p01_1 + p10_1)\n",
    "\n",
    "        # (b) Mean and covariance parameters for x3 and x4\n",
    "        self.__mus['x3'] = {0: np.mean(x3_0, axis = 0).item(), 1: np.mean(x3_1, axis = 0).item()}\n",
    "        self.__mus['x4'] = {0: np.mean(x4_0, axis = 0).item(), 1: np.mean(x4_1, axis = 0).item()}\n",
    "        self.__covs[0] = np.array([\n",
    "            [np.sum(np.square(x3_0 - self.__mus['x3'][0])) / N, np.sum((x3_0 - self.__mus['x3'][0]) * (x4_0 - self.__mus['x4'][0])) / N],\n",
    "            [np.sum((x4_0 - self.__mus['x4'][0]) * (x3_0 - self.__mus['x3'][0])) / N, np.sum(np.square(x4_0 - self.__mus['x4'][0])) / N]\n",
    "            ])\n",
    "        self.__covs[1] = np.array([\n",
    "            [np.sum(np.square(x3_1 - self.__mus['x3'][1])) / N, np.sum((x3_1 - self.__mus['x3'][1]) * (x4_1 - self.__mus['x4'][1])) / N],\n",
    "            [np.sum((x4_1 - self.__mus['x4'][1]) * (x3_1 - self.__mus['x3'][1])) / N, np.sum(np.square(x4_1 - self.__mus['x4'][1])) / N]\n",
    "            ])\n",
    "\n",
    "        # (c) Prior parameter\n",
    "        self.__priors[0] = len(y[y == 0]) / y.shape[0]\n",
    "        self.__priors[1] = 1 - self.__priors[0]\n",
    "\n",
    "        pass  # remove this line\n",
    "    def bivariate_gaussian(x1, x2, mean1, mean2, cov_matrix):\n",
    "        \"\"\"\n",
    "        std_dev_1 = np.sqrt(cov_matrix[0,0])\n",
    "        std_dev_2 = np.sqrt(cov_matrix[1,1])\n",
    "        rho = cov_matrix[0,1] / (mean1 * mean2)\n",
    "        k = 1 / (2 * np.pi * mean1 * mean2 * np.sqrt(1 - rho ** 2))\n",
    "        z = () - (2 * rho(x1 - mean1)) / (mean1 * mean2) + (x2 - mean2)**2 / (mean2 ** 2)\n",
    "        np.exp()\n",
    "        \"\"\"\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Inputs:\n",
    "            X - np.array with shape (num_examples_test, 4)\n",
    "        \n",
    "        Outputs:\n",
    "            ypred - np.array with shape (num_examples_test,)\n",
    "            posteriors - np.array with shape (num_examples_test, 2)\n",
    "        '''\n",
    "        #posteriors = []\n",
    "        \n",
    "        pass  # remove this line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2bf886",
   "metadata": {},
   "source": [
    "**N.B.:** In both a) and b), you should avoid for loops as much as possible by using vectorized NumPy operations and broadcasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe3261d",
   "metadata": {},
   "source": [
    "**a)** Implement the `fit` method, which receives as input two `np.array`s:\n",
    "- `X`, which contains the 4-dimensional training input examples $\\boldsymbol{x}^{(i)}$, one per row;\n",
    "- `y`, which contains the corresponding training labels $y^{(i)} \\in \\{0,1\\}$, one per row.\n",
    "\n",
    "This method should compute the maximum likelihood estimates of the model parameters and store them as class attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314b8621",
   "metadata": {},
   "source": [
    "**b)** Implement the `predict` method, which receives as input one `np.array`:\n",
    "- `X`, which contains the 4-dimensional examples $\\boldsymbol{x}^{(i)}$ to be classified, one per row.\n",
    "\n",
    "This function should return two `np.array`s:\n",
    "- `ypred`, which should contain the labels predicted for each $\\boldsymbol{x}^{(i)}$, one per row.\n",
    "- `posteriors`, which should contain the posterior probabilities of each class given each $\\boldsymbol{x}^{(i)}$, one per row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e3bcac",
   "metadata": {},
   "source": [
    "If you have solved a) and b) correctly, the code below should run without errors and the reported test accuracy should be higher than 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b0780e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1072961373390558 0.24034334763948498 0.44635193133047213 0.20600858369098707\n",
      "0.2994011976047904 0.10778443113772455 0.3652694610778443 0.22754491017964074\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37224/2969192505.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# get the predictions on the test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mypred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposteriors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Example 0:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'  posteriors ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposteriors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# read the data from file\n",
    "data = np.genfromtxt('ex1_data.txt')\n",
    "X, y = data[:, 0:4], data[:, 4].astype(int)\n",
    "\n",
    "# use the first 400 lines for training and the remaining 100 lines for testing\n",
    "Xtrain, ytrain = X[0:400], y[0:400]\n",
    "Xtest, ytest = X[400:], y[400:]\n",
    "\n",
    "# instantiate the classifier and train it\n",
    "classifier = Classifier()\n",
    "classifier.fit(Xtrain, ytrain)\n",
    "\n",
    "# get the predictions on the test data\n",
    "ypred, posteriors = classifier.predict(Xtest)\n",
    "print('Example 0:')\n",
    "print('  posteriors =', posteriors[0])\n",
    "print('  predicted class =', ypred[0])\n",
    "print('  ground-truth class =', ytest[0])\n",
    "print()\n",
    "print('Example 1:')\n",
    "print('  posteriors =', posteriors[1])\n",
    "print('  predicted class =', ypred[1])\n",
    "print('  ground-truth class =', ytest[1])\n",
    "print()\n",
    "print('Example 2:')\n",
    "print('  posteriors =', posteriors[2])\n",
    "print('  predicted class =', ypred[2])\n",
    "print('  ground-truth class =', ytest[2])\n",
    "print()\n",
    "\n",
    "# compute the accuracy on the test set\n",
    "acc = np.mean(ypred == ytest)\n",
    "print(f'Test accuracy = {100.*acc:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ca18f0",
   "metadata": {},
   "source": [
    "## 2. Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc50814",
   "metadata": {},
   "source": [
    "Consider the `heightWeightData.txt` dataset that you have used in the Lab classes. You will use this data to build a Logistic Regression classifier that predicts the sex of an individual given their height and weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1201ad8f",
   "metadata": {},
   "source": [
    "**a)** Train a Logistic Regression classifier **using only the first 160 rows** of the dataset as training data. You may use Scikit-Learn (`sklearn.linear_model.LogisticRegression`). **Print the values of the learned parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596173f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.15294769 -0.10445359]] [34.00288831]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('heightWeightData.txt', sep = \",\", header = None)\n",
    "X_train, Y_train, X_test, Y_test = data.iloc[:160,1:], data.iloc[:160,0], data.iloc[160:,1:], data.iloc[160:,0]\n",
    "\n",
    "# 1. Creating the model\n",
    "model = LogisticRegression(random_state = 0) \n",
    "\n",
    "# 2. Training the model\n",
    "clf = model.fit(X_train, Y_train) \n",
    "\n",
    "# Printing the coefficients\n",
    "print(clf.coef_, clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2465b356",
   "metadata": {},
   "source": [
    "**b)** Compute the predictions of your model in the remaining 50 rows of the dataset and report the classification accuracy of your model in this test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984e8b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for the last 50 rows: \n",
      "[1 2 1 2 2 2 2 1 2 1 2 2 2 1 2 2 2 2 2 2 1 1 2 2 1 2 2 2 1 2 2 1 2 2 2 2 1\n",
      " 2 2 1 2 2 1 2 1 1 1 1 2 1]\n",
      "The classification accuracy is 92.0%\n"
     ]
    }
   ],
   "source": [
    "preds = clf.predict(X_test)\n",
    "print(f'Predictions for the last 50 rows: \\n{preds}')\n",
    "print(f'The classification accuracy is {100 * model.score(X_test, Y_test)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593edb81",
   "metadata": {},
   "source": [
    "**c)** Using the parameter values printed in a), write the equation of the decision boundary of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19922303",
   "metadata": {},
   "source": [
    "$Y = W^TX + W_0$,\n",
    "where \n",
    "$W^T=[-0.15294769, -0.10445359]$ \n",
    "and $W_0 = 34.00288831$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
