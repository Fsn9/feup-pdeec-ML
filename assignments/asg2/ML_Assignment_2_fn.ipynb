{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32620509",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1993cc",
   "metadata": {},
   "source": [
    "Student: Francisco Neves (up201404576)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03c866e",
   "metadata": {},
   "source": [
    "## 1. Generative classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38343624",
   "metadata": {},
   "source": [
    "Consider a classification problem with a target variable $y \\in \\{0, 1\\}$ and input features $\\boldsymbol{x} = (x_1\\; x_2\\; x_3\\; x_4)^T$, where $x_1 \\in \\{0, 1\\}$, $x_2 \\in \\{0, 1\\}$, and $(x_3, x_4) \\in \\mathbb{R}^2$. Further assume that:\n",
    "- $(x_1, x_2)$ is conditionally independent of $(x_3, x_4)$ given $y$;\n",
    "- $x_1$ and $x_2$ are **dependent** given $y$;\n",
    "- $x_3$ and $x_4$ are **dependent** given $y$;\n",
    "- the conditional distributions of $(x_3, x_4)$ given $y$ are Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c2977c",
   "metadata": {},
   "source": [
    "### 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6081895",
   "metadata": {},
   "source": [
    "**a)** Enumerate the parameters of the MAP classifier: $$\\hat{y} = \\text{arg} \\max_{y \\in \\{0, 1\\}} p(y)p(\\boldsymbol{x} \\mid y),$$ and indicate the dimension of each parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc08105",
   "metadata": {},
   "source": [
    "___\n",
    "Before estimating the parameters, by assuming partial independence between $(x_1,x_2)$ and $(x_3,x_4)$ the likelihood is described by:\n",
    "$\\begin{equation}\n",
    "p\\left(\\begin{bmatrix}\n",
    "x_{1} \\\\\n",
    "x_{2} \\\\\n",
    "x_{3} \\\\\n",
    "x_{4}\n",
    "\\end{bmatrix}\\middle\\vert y\\right) = \n",
    "p\\left(\\begin{bmatrix}\n",
    "x_{1} \\\\\n",
    "x_{2}\n",
    "\\end{bmatrix}\\middle\\vert y\\right)\n",
    "p\\left(\\begin{bmatrix}\n",
    "x_{3} \\\\\n",
    "x_{4}\n",
    "\\end{bmatrix}\\middle\\vert y\\right).\n",
    "\\end{equation}$\n",
    "\n",
    "Following, we enumerate the different parameters by breaking the problem in three parts (a), (b) and (c).\n",
    "### (a) Parameters for $(x_1,x_2)$\n",
    "For the jointed $(x_1, x_2)$ features, we have six parameters in total: 3 parameters for $p\\left(\\begin{bmatrix} x_{1} \\\\ x_{2}\\end{bmatrix}\\middle\\vert y = 0 \\right)$ and 3 parameters for $p\\left(\\begin{bmatrix} x_{1} \\\\ x_{2}\\end{bmatrix}\\middle\\vert y = 1\\right)$. Since $x_1 \\in \\{0,1\\}$ and $x_2 \\in \\{0,1\\}$ and they are dependent between each other given $y$, one needs to consider all the possible four joint configuration using 2 bits of information, such as $[0,0], [0,1], [1,0], [1,1]$. At least three parameters are enough to estimate for each $p\\left(\\begin{bmatrix} x_{1} \\\\ x_{2}\\end{bmatrix}\\middle\\vert y_i\\right)$, since a fourth is indirectly deduced from the others, such as,\n",
    "$\\begin{equation}\n",
    "  p\\left(\\begin{bmatrix} x_{1} = 0 \\\\ x_{2} = 0\\end{bmatrix}\\middle\\vert y_i\\right) + p\\left(\\begin{bmatrix} x_{1} = 0 \\\\ x_{2} = 1\\end{bmatrix}\\middle\\vert y_i \\right) + p\\left(\\begin{bmatrix} x_{1} = 1 \\\\ x_{2} = 0\\end{bmatrix}\\middle\\vert y_i \\right) + p\\left(\\begin{bmatrix} x_{1} = 1 \\\\ x_{2} = 1\\end{bmatrix}\\middle\\vert y_i \\right) = 1,\n",
    "\\end{equation}$\n",
    "where if $p\\left(\\begin{bmatrix} x_{1} = 1 \\\\ x_{2} = 1\\end{bmatrix}\\middle\\vert y_i \\right)$ is the one not considered, the computation is as follows:\n",
    "$\\begin{equation}\n",
    "p\\left(\\begin{bmatrix} x_{1} = 1 \\\\ x_{2} = 1\\end{bmatrix}\\middle\\vert y_i \\right) = 1 - \\left(p\\left(\\begin{bmatrix} x_{1} = 0 \\\\ x_{2} = 0\\end{bmatrix}\\middle\\vert y_i\\right) + p\\left(\\begin{bmatrix} x_{1} = 0 \\\\ x_{2} = 1\\end{bmatrix}\\middle\\vert y_i \\right) + p\\left(\\begin{bmatrix} x_{1} = 1 \\\\ x_{2} = 0\\end{bmatrix}\\middle\\vert y_i \\right)\\right).\n",
    "\\end{equation}$ \n",
    "Hence, the six parameters are the following:\n",
    "* 3 parameters for $p\\left(\\begin{bmatrix} x_{1} \\\\ x_{2}\\end{bmatrix}\\middle\\vert y = 0 \\right)$: \n",
    "  * $p\\left(\\begin{bmatrix} x_{1} = 0 \\\\ x_{2} = 0\\end{bmatrix}\\middle\\vert y = 0 \\right)$\n",
    "  * $p\\left(\\begin{bmatrix} x_{1} = 0 \\\\ x_{2} = 1\\end{bmatrix}\\middle\\vert y = 0 \\right)$\n",
    "  * $p\\left(\\begin{bmatrix} x_{1} = 1 \\\\ x_{2} = 0\\end{bmatrix}\\middle\\vert y = 0 \\right)$\n",
    "* 3 parameters for $p\\left(\\begin{bmatrix} x_{1} \\\\ x_{2}\\end{bmatrix}\\middle\\vert y = 1 \\right)$:\n",
    "  * $p\\left(\\begin{bmatrix} x_{1} = 0 \\\\ x_{2} = 0\\end{bmatrix}\\middle\\vert y = 1 \\right)$\n",
    "  * $p\\left(\\begin{bmatrix} x_{1} = 0 \\\\ x_{2} = 1\\end{bmatrix}\\middle\\vert y = 1 \\right)$\n",
    "  * $p\\left(\\begin{bmatrix} x_{1} = 1 \\\\ x_{2} = 0\\end{bmatrix}\\middle\\vert y = 1 \\right)$\n",
    "\n",
    "\n",
    "### (b) Parameters for $(x_3, x_4)$\n",
    "For the jointed $(x_3, x_4)$ features, we have ten parameters, as follows: \n",
    "* 4 means: $\\mu_{x_3}=\\begin{bmatrix}\\mu_{x_3}^{y=0} \\\\ \\mu_{x_3}^{y=1}\\end{bmatrix}$ and $\\mu_{x_4}=\\begin{bmatrix}\\mu_{x_4}^{y=0} \\\\ \\mu_{x_4}^{y=1}\\end{bmatrix}$\n",
    "* 6 covariance elements: \n",
    "$\\begin{bmatrix}\n",
    "\\sigma^{2}(x_3,x_3)_{y = 0} & \\sigma^{2}(x_3,x_4)_{y = 0} \\\\\n",
    "\\cdots & \\sigma^{2}(x_4,x_4)_{y = 0}\n",
    "\\end{bmatrix}$ and \n",
    "$\\begin{bmatrix}\\sigma^{2}(x_3,x_3)_{y = 1} & \\sigma^{2}(x_3,x_4)_{y = 1} \\\\ \\cdots & \\sigma^{2}(x_4,x_4)_{y = 1}\\end{bmatrix}$, where $\\sigma^{2}(x_4,x_3)_{y = 1}$ and $\\sigma^{2}(x_4,x_3)_{y = 0}$ are omitted, since  $\\sigma^{2}(x_3,x_4)_{y = 0} = \\sigma^{2}(x_4,x_3)_{y = 0}$ and $\\sigma^{2}(x_3,x_4)_{y = 1} = \\sigma^{2}(x_4,x_3)_{y = 1}$ because the covariance matrix is symmetric.\n",
    "\n",
    "### (c) Prior parameters\n",
    "Finally, we have to estimate just one prior as follows: $p(y=0)$. Since $1 - p(y=0) = p(y=1)$, knowing one of them deduces the other.\n",
    "\n",
    "### Number of parameters\n",
    "Hence, in total the model has 17 parameters, such as: 6 parameters from the part (a) plus 10 parameters from the part (b) plus 1 parameter from the part (c).\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41873490",
   "metadata": {},
   "source": [
    "**b)** Given a dataset $\\{(\\boldsymbol{x}^{(i)}, y^{(i)})\\}_{i=1}^n$, write the expressions for the maximum likelihood estimates of the parameters enumerated in the previous question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2a6e5d",
   "metadata": {},
   "source": [
    "Let $D = \\{(\\boldsymbol{x}^{(i)}, y^{(i)})\\}_{i=1}^n$ be the dataset, $N_0 = |D\\{y = 0\\}|$ and $N_1 = |D\\{y = 1\\}|$ be the lengths of a subset of the original dataset where $y=0$ and $y=1$, respectively.\n",
    "\n",
    "### (a) MLE for $(x_1,x_2)$ parameters\n",
    "* $p\\left(\\begin{bmatrix} x_{1} = 0 \\\\ x_{2} = 0\\end{bmatrix}\\middle\\vert y = 0 \\right) = \\frac{\\#D\\{x_1 = 0 \\wedge x_2 = 0\\}}{N_0}$\n",
    "* $p\\left(\\begin{bmatrix} x_{1} = 0 \\\\ x_{2} = 1\\end{bmatrix}\\middle\\vert y = 0 \\right) = \\frac{\\#D\\{x_1 = 0 \\wedge x_2 = 1\\}}{N_0}$\n",
    "* $p\\left(\\begin{bmatrix} x_{1} = 1 \\\\ x_{2} = 0\\end{bmatrix}\\middle\\vert y = 0 \\right) = \\frac{\\#D\\{x_1 = 1 \\wedge x_2 = 0\\}}{N_0}$\n",
    "* $p\\left(\\begin{bmatrix} x_{1} = 0 \\\\ x_{2} = 0\\end{bmatrix}\\middle\\vert y = 1 \\right) = \\frac{\\#D\\{x_1 = 0 \\wedge x_2 = 0\\}}{N_1}$\n",
    "* $p\\left(\\begin{bmatrix} x_{1} = 0 \\\\ x_{2} = 1\\end{bmatrix}\\middle\\vert y = 1 \\right) = \\frac{\\#D\\{x_1 = 0 \\wedge x_2 = 1\\}}{N_1}$\n",
    "* $p\\left(\\begin{bmatrix} x_{1} = 1 \\\\ x_{2} = 0\\end{bmatrix}\\middle\\vert y = 1 \\right) = \\frac{\\#D\\{x_1 = 1 \\wedge x_2 = 0\\}}{N_1}$\n",
    "\n",
    "### (b) MLE for $(x_3,x_4)$ parameters\n",
    "* $\\mu_{x_3}^{y=0} = \\frac{1}{N_0}\\sum\\limits_{i=1}^{n}x_3^i$\n",
    "* $\\mu_{x_4}^{y=0} = \\frac{1}{N_0}\\sum\\limits_{i=1}^{n}x_4^i$\n",
    "* $\\mu_{x_3}^{y=1} = \\frac{1}{N_1}\\sum\\limits_{i=1}^{n}x_3^i$\n",
    "* $\\mu_{x_4}^{y=1} = \\frac{1}{N_1}\\sum\\limits_{i=1}^{n}x_4^i$\n",
    "* $\\sigma^{2}(x_3,x_3)_{y = 0} = \\frac{1}{n}\\sum\\limits_{i=1}^{n}(x_3^i - \\mu_{x_3}^{y=0})(x_3^i - \\mu_{x_3}^{y=0})^T$\n",
    "* $\\sigma^{2}(x_3,x_4)_{y = 0} = \\frac{1}{n}\\sum\\limits_{i=1}^{n}(x_3^i - \\mu_{x_3}^{y=0})(x_4^i - \\mu_{x_4}^{y=0})^T$\n",
    "* $\\sigma^{2}(x_4,x_4)_{y = 0} = \\frac{1}{n}\\sum\\limits_{i=1}^{n}(x_4^i - \\mu_{x_4}^{y=0})(x_4^i - \\mu_{x_4}^{y=0})^T$\n",
    "* $\\sigma^{2}(x_3,x_3)_{y = 1} = \\frac{1}{n}\\sum\\limits_{i=1}^{n}(x_3^i - \\mu_{x_3}^{y=1})(x_3^i - \\mu_{x_3}^{y=1})^T$\n",
    "* $\\sigma^{2}(x_3,x_4)_{y = 1} = \\frac{1}{n}\\sum\\limits_{i=1}^{n}(x_3^i - \\mu_{x_3}^{y=1})(x_4^i - \\mu_{x_4}^{y=1})^T$\n",
    "* $\\sigma^{2}(x_4,x_4)_{y = 1} = \\frac{1}{n}\\sum\\limits_{i=1}^{n}(x_4^i - \\mu_{x_4}^{y=1})(x_4^i - \\mu_{x_4}^{y=1})^T$\n",
    "\n",
    "### (c) MLE for prior parameter\n",
    "* $p(y=0) = \\frac{\\#D\\{y=0\\}}{|D|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c45eaf5",
   "metadata": {},
   "source": [
    "### 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba5cb63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9823efc7",
   "metadata": {},
   "source": [
    "Now, you will implement this classifier in Python. The classifier skeleton is provided below in the class `Classifier`. You may implement additional auxiliary methods that you find useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12ed9dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Inputs:\n",
    "            X - np.array with shape (num_examples_train, 4)\n",
    "            y - np.array with shape (num_examples_train,)\n",
    "        '''\n",
    "        # Class attributes\n",
    "        self.__priors = {}\n",
    "        self.__mus = {}\n",
    "        self.__covs = {}\n",
    "        self.__p_x1_x2 = {0: {(0,0): None, (0,1): None, (1,0): None, (1,1): None}, 1: {(0,0): None, (0,1): None, (1,0): None, (1,1): None}}\n",
    "\n",
    "        # Useful variables\n",
    "        Xy = np.concatenate((X, y.reshape(-1,1)), axis = 1)\n",
    "        N = X.shape[0]\n",
    "        Xy_0, Xy_1 = Xy[Xy[:,-1] == 0], Xy[Xy[:,-1] == 1]\n",
    "        x00_0 = Xy_0[(Xy_0[:,0] == 0) & (Xy_0[:,1] == 0)]\n",
    "        x01_0 = Xy_0[(Xy_0[:,0] == 0) & (Xy_0[:,1] == 1)]\n",
    "        x10_0 = Xy_0[(Xy_0[:,0] == 1) & (Xy_0[:,1] == 0)]\n",
    "        x11_0 = Xy_0[(Xy_0[:,0] == 1) & (Xy_0[:,1] == 1)]\n",
    "        x00_1 = Xy_1[(Xy_1[:,0] == 0) & (Xy_1[:,1] == 0)]\n",
    "        x01_1 = Xy_1[(Xy_1[:,0] == 0) & (Xy_1[:,1] == 1)]\n",
    "        x10_1 = Xy_1[(Xy_1[:,0] == 1) & (Xy_1[:,1] == 0)]\n",
    "        x11_1 = Xy_1[(Xy_1[:,0] == 1) & (Xy_1[:,1] == 1)]\n",
    "        x3_0, x3_1, x4_0, x4_1 = Xy_0[:,2].reshape(-1,1), Xy_1[:,2].reshape(-1,1), Xy_0[:,3].reshape(-1,1), Xy_1[:,3].reshape(-1,1)\n",
    "\n",
    "        # (a) Conditional discrete probabilities for x1 and x2\n",
    "        self.__p_x1_x2[0][(0,0)], self.__p_x1_x2[0][(0,1)], self.__p_x1_x2[0][(1,0)] = x00_0.shape[0] / Xy_0.shape[0], x01_0.shape[0] / Xy_0.shape[0], x10_0.shape[0] / Xy_0.shape[0]\n",
    "        self.__p_x1_x2[0][(1,1)] = 1 - (self.__p_x1_x2[0][(0,0)] + self.__p_x1_x2[0][(0,1)] + self.__p_x1_x2[0][(1,0)])\n",
    "\n",
    "        self.__p_x1_x2[1][(0,0)], self.__p_x1_x2[1][(0,1)], self.__p_x1_x2[1][(1,0)] = x00_1.shape[0] / Xy_1.shape[0], x01_1.shape[0] / Xy_1.shape[0], x10_1.shape[0] / Xy_1.shape[0]\n",
    "        self.__p_x1_x2[1][(1,1)] = 1 - (self.__p_x1_x2[1][(0,0)] + self.__p_x1_x2[1][(0,1)] + self.__p_x1_x2[1][(1,0)])\n",
    "\n",
    "        # (b) Mean and covariance parameters for x3 and x4\n",
    "        self.__mus = {0: np.array([np.mean(x3_0, axis = 0).item(), np.mean(x4_0, axis = 0).item()]).reshape(-1,1), \n",
    "            1: np.array([np.mean(x3_1, axis = 0).item(), np.mean(x4_1, axis = 0).item()]).reshape(-1,1)}\n",
    "        self.__covs[0] = np.cov(np.concatenate((x3_0.T, x4_0.T), axis = 0))\n",
    "        self.__covs[1] = np.cov(np.concatenate((x3_1.T, x4_1.T), axis = 0))\n",
    "\n",
    "        # (c) Prior parameters\n",
    "        self.__priors[0] = len(y[y == 0]) / y.shape[0]\n",
    "        self.__priors[1] = 1 - self.__priors[0]\n",
    "\n",
    "    # Receives x with shape (num_samples, num_features)\n",
    "    @staticmethod\n",
    "    def multi_variate_gaussian(x, mean, cov_matrix):\n",
    "        k = 1 / ((2 * np.pi) ** (cov_matrix.shape[0] * 0.5)) * np.linalg.det(cov_matrix) ** (-0.5)\n",
    "        z = np.dot(np.dot(-0.5 * (x.T - mean).T, np.linalg.inv(cov_matrix)), (x.T - mean))\n",
    "        return np.diag((k * np.exp(z))).reshape(-1,1)\n",
    "\n",
    "    # Transforms a np.array with shape (num_examples, 2) into a np.array with shape (num_examples, 1) with discrete probabilities\n",
    "    def binomial_discrete_gaussian(self, X, class_idx):\n",
    "        return np.apply_along_axis(func1d = lambda X : self.__p_x1_x2[class_idx][(X[0], X[1])], axis = 1, arr =  X).reshape(-1,1)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Inputs:\n",
    "            X - np.array with shape (num_examples_test, 4)\n",
    "        \n",
    "        Outputs:\n",
    "            ypred - np.array with shape (num_examples_test,)\n",
    "            posteriors - np.array with shape (num_examples_test, 2)\n",
    "        '''\n",
    "        likelihood_x1_x2_0 = self.binomial_discrete_gaussian(X[:,:2].astype(np.int16), class_idx = 0)\n",
    "        likelihood_x1_x2_1 = self.binomial_discrete_gaussian(X[:,:2].astype(np.int16), class_idx = 1)\n",
    "        likelihood_x3_x4_0 = self.multi_variate_gaussian(X[:,2:], self.__mus[0], self.__covs[0])\n",
    "        likelihood_x3_x4_1 = self.multi_variate_gaussian(X[:,2:], self.__mus[1], self.__covs[1])\n",
    "        posterior0 = likelihood_x1_x2_0 * likelihood_x3_x4_0 * self.__priors[0]\n",
    "        posterior1 = likelihood_x1_x2_1 * likelihood_x3_x4_1 * self.__priors[1]\n",
    "        p_X = posterior0 + posterior1\n",
    "        posteriors = np.concatenate([posterior0 / p_X, posterior1 / p_X], axis = 1)\n",
    "        return np.argmax(posteriors, axis = 1), posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2bf886",
   "metadata": {},
   "source": [
    "**N.B.:** In both a) and b), you should avoid for loops as much as possible by using vectorized NumPy operations and broadcasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe3261d",
   "metadata": {},
   "source": [
    "**a)** Implement the `fit` method, which receives as input two `np.array`s:\n",
    "- `X`, which contains the 4-dimensional training input examples $\\boldsymbol{x}^{(i)}$, one per row;\n",
    "- `y`, which contains the corresponding training labels $y^{(i)} \\in \\{0,1\\}$, one per row.\n",
    "\n",
    "This method should compute the maximum likelihood estimates of the model parameters and store them as class attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314b8621",
   "metadata": {},
   "source": [
    "**b)** Implement the `predict` method, which receives as input one `np.array`:\n",
    "- `X`, which contains the 4-dimensional examples $\\boldsymbol{x}^{(i)}$ to be classified, one per row.\n",
    "\n",
    "This function should return two `np.array`s:\n",
    "- `ypred`, which should contain the labels predicted for each $\\boldsymbol{x}^{(i)}$, one per row.\n",
    "- `posteriors`, which should contain the posterior probabilities of each class given each $\\boldsymbol{x}^{(i)}$, one per row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e3bcac",
   "metadata": {},
   "source": [
    "If you have solved a) and b) correctly, the code below should run without errors and the reported test accuracy should be higher than 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0780e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0:\n",
      "  posteriors = [0.98215382 0.01784618]\n",
      "  predicted class = 0\n",
      "  ground-truth class = 0\n",
      "\n",
      "Example 1:\n",
      "  posteriors = [0.76798406 0.23201594]\n",
      "  predicted class = 0\n",
      "  ground-truth class = 1\n",
      "\n",
      "Example 2:\n",
      "  posteriors = [0.03193052 0.96806948]\n",
      "  predicted class = 1\n",
      "  ground-truth class = 1\n",
      "\n",
      "Test accuracy = 92.0%\n"
     ]
    }
   ],
   "source": [
    "# read the data from file\n",
    "data = np.genfromtxt('ex1_data.txt')\n",
    "X, y = data[:, 0:4], data[:, 4].astype(int)\n",
    "\n",
    "# use the first 400 lines for training and the remaining 100 lines for testing\n",
    "Xtrain, ytrain = X[0:400], y[0:400]\n",
    "Xtest, ytest = X[400:], y[400:]\n",
    "\n",
    "# instantiate the classifier and train it\n",
    "classifier = Classifier()\n",
    "classifier.fit(Xtrain, ytrain)\n",
    "\n",
    "# get the predictions on the test data\n",
    "ypred, posteriors = classifier.predict(Xtest)\n",
    "print('Example 0:')\n",
    "print('  posteriors =', posteriors[0])\n",
    "print('  predicted class =', ypred[0])\n",
    "print('  ground-truth class =', ytest[0])\n",
    "print()\n",
    "print('Example 1:')\n",
    "print('  posteriors =', posteriors[1])\n",
    "print('  predicted class =', ypred[1])\n",
    "print('  ground-truth class =', ytest[1])\n",
    "print()\n",
    "print('Example 2:')\n",
    "print('  posteriors =', posteriors[2])\n",
    "print('  predicted class =', ypred[2])\n",
    "print('  ground-truth class =', ytest[2])\n",
    "print()\n",
    "\n",
    "# compute the accuracy on the test set\n",
    "acc = np.mean(ypred == ytest)\n",
    "print(f'Test accuracy = {100.*acc:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ca18f0",
   "metadata": {},
   "source": [
    "## 2. Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc50814",
   "metadata": {},
   "source": [
    "Consider the `heightWeightData.txt` dataset that you have used in the Lab classes. You will use this data to build a Logistic Regression classifier that predicts the sex of an individual given their height and weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1201ad8f",
   "metadata": {},
   "source": [
    "**a)** Train a Logistic Regression classifier **using only the first 160 rows** of the dataset as training data. You may use Scikit-Learn (`sklearn.linear_model.LogisticRegression`). **Print the values of the learned parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "596173f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights w1 and w2:[[-0.15294769 -0.10445359]] and the bias w0: [34.00288831]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('heightWeightData.txt', sep = \",\", header = None)\n",
    "X_train, Y_train, X_test, Y_test = data.iloc[:160,1:], data.iloc[:160,0], data.iloc[160:,1:], data.iloc[160:,0]\n",
    "\n",
    "# 1. Creating the model\n",
    "model = LogisticRegression(random_state = 0) \n",
    "\n",
    "# 2. Training the model\n",
    "clf = model.fit(X_train, Y_train) \n",
    "\n",
    "# Printing the coefficients\n",
    "print(f'The weights w1 and w2:{clf.coef_} and the bias w0: {clf.intercept_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2465b356",
   "metadata": {},
   "source": [
    "**b)** Compute the predictions of your model in the remaining 50 rows of the dataset and report the classification accuracy of your model in this test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "984e8b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for the last 50 rows: \n",
      "[1 2 1 2 2 2 2 1 2 1 2 2 2 1 2 2 2 2 2 2 1 1 2 2 1 2 2 2 1 2 2 1 2 2 2 2 1\n",
      " 2 2 1 2 2 1 2 1 1 1 1 2 1]\n",
      "The classification accuracy is 92.0%\n"
     ]
    }
   ],
   "source": [
    "print(f'Predictions for the last 50 rows: \\n{clf.predict(X_test)}')\n",
    "print(f'The classification accuracy is {100 * model.score(X_test, Y_test)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593edb81",
   "metadata": {},
   "source": [
    "**c)** Using the parameter values printed in a), write the equation of the decision boundary of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19922303",
   "metadata": {},
   "source": [
    "$Y = W^TX + W_0$,\n",
    "where \n",
    "$W^T=[-0.15294769, -0.10445359]$ \n",
    "and $W_0 = 34.00288831$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
